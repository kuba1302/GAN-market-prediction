% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information
\usepackage{adjustbox}
\usepackage{graphicx} % support the \includegraphics command and options
\usepackage{amsfonts}
% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{pgf}
%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{titlesec}
\usepackage{float}
\usepackage[T1]{fontenc}
\usepackage{indentfirst} 
%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{Stock price prediction using GAN and BERT <nie jestem w stanie słowa rozczytać> case of game industry companies}
\author{Jakub Wujec}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\newenvironment{conditions}
  {\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}={}$} l}}
  {\end{tabular}\par\vspace{\belowdisplayskip}}
\begin{document}
\maketitle


\begin{abstract} In this article we propose the use of the Generative adversarial networks (GAN) model to predict stock market behavior. We have also developed an investment strategy that uses the results of the model. We will use Technical Analysis and Sentiment Analysis as explanatory variables for our model. Our study was conducted on four companies in the game industry. This industry was chosen by us because of the potential impact of a company's customer reviews on its valuation. We investigate this by analyzing the sentiment of selected comments containing pre-prepared keywords. The sentiment is then calculated using the NLP model created by Google - BERT. GAN then tries to predict the future valuation of a given company using the variables mentioned above. This valuation is then used to provide buy or sell signals.  
\end{abstract}

\section{Introduction}
In recent years, the stock market has become a place of fierce competition for the best model to predict future prices and consequently, to make money \cite{ml-in-stock}. 
Significant influence on this has the facilitation of access to computing power, allowing the use of algorithms such as neural networks, without worrying about hardware limitations \cite{ml-compute}. Moreover, a great number of stock exchanges make their APIs available, thus enabling real-time algo-trading. All of this has led to a great increase in recent research on the use of new machine learning models for stock market price prediction \cite{ml-studies}.  

Initially, all kinds of econometric models such as ARIMA were used for this purpose. The development of technology has allowed increasingly complex machine learning models to be applied to time series prediction. Deep neural networks models are also increasingly used. Their architecture is also evolving, which means that there are many different types of neural networks. Mostly recurrent neural network (RNN) models such as GRU and LSTM are used. 

Recently, a new architecture has emerged, which we will explore in our study. It is Generative Adversarial Networks introduced in 2014 by J. Goodfellow \cite{gan1}. They were initially intended to generate synthetic images. However, later research has shown that they can be successfully used to generate future stock market valuations. Sonkiya et al \cite{s-gan} used S-GAN with sentiment analysis of financial news as an input for generator. The finBERT model, trained on financial data, was used for sentiment analysis. They managed to create a model that wins against traditional approaches such as ARIMA, LSTM and GRU. Kumar et al  \cite{gan-stock} used phase-space reconstruction (PSR) and GAN and compared it to LSTM approach with PSR. They have tested multiple time window sizes and epochs and managed to reduce the training time of the system compared to traditional approaches while maintaining satisfactory results. 

Staffini \cite{gan-cnn} proposed a DCGAN architecture with CNN-BiLSTM discriminator and CNN generator. The models were compared with traditional approaches such as ARIMAX-SVR, Random Forest, LSTM on 10 different stock market instruments. Both in single-step and in the multi-step prediction, proposed DCGAN outperformed typical approaches. Zhang et al \cite{gan-zhang} proposed GAN with LSTM as generator and MLP as discriminator. They have used pure financial data from yahoo finance as generator input. Then, proposed model have been compared with ANN, SVR and LSTM. In this case, GAN was also found to be superior to the traditional approach.  

Jiang \cite{gan-Jjang} has proposed a novel GAN architecture. It is called SF-GAN and consist of 
State Frequency Memory Neural Network (SFM) as a generator and CNN as a discriminator. 
He was able to achieve a significant improvement in the trend prediction of a financial instrument and minimise the prediction error.  Lin et al \cite{gan-lin} has proposed two versions of GAN for time series. First one was similar to Sonkiya S-GAN with GRU as generator and CNN as discriminator. They have extended this idea and included WGAN-GP, a Wasserstein GAN with Gradient Penalty. Compared with GAN, WGAN-GP doesn't have a sigmoid function and it outputs a scalar value instead of probability. 

Analysis of results if he aforementioned researches it may be stated that gan algorithms give very promising results. These data the results of the previously mentioned researchers, for example P. Sonkiya's et al S-GAN model menaged to get RMSE 62 $\% $ smaller then LSTM, getting for S-GAN and LSTM 1.827 and 2.939 respectively. Therefore we would like to apply GAN in predicting stocks. 

We will use yahoo finance to download stock market data. From these we then select the indicators used to predict the explanatory variable, which in our case is the closing price of a given candlestick. This data is also used to calculate technical analysis indicators. These are simple moving average (SMA), exponential moving average (EMA), weighted moving average (WMA), bollinger bands (BB) and moving average convergence divergence (MACD). 

Another source of information of our model will be sentiment analysis. Sentiment is one way of gauging people's feelings about a company or its products. It is even more important in the game industry, because the opinion of players expressed by posts on forums and the decision to buy a particular game  significantly affects the finance results of the company. To download text data, we will use the Reddit portal - one of the largest Internet forums of this kind. It also has subforums that allow aggregation of information by topic. This will allow us to retrieve comments on selected keywords from a site dedicated only to games. These keywords will be the names of companies and their most popular games.

Google's BERT model will be used to analyse sentiment. It is a model pre-trained on millions of texts and considered a state-of-the-art model in the field of NLP. 
We then use stock market data, technical analysis indicators and sentiment analysis to try to predict the closing price of a given company the next day. For the prediction of one day we use explanatory variables from the previous 30 days. With the model, we tried to prepare an investment strategy that allows to make buying and selling decisions. We have taken into account transaction costs, the different cut-off points required for a trade and the possibility of potential short selling. 

The following hypotheses were verified during our study: 

 \begin{flushleft}\textbf{Hypothesis 1 (H1)} \textit{Using data from sentiment analysis incrase model predictions quality measured by MAE the behaviour of the model }  \end{flushleft}
 \begin{flushleft}\textbf{Hypothesis 2 (H2)} \textit{The use of technical analysis indicators positively influences the behaviour of the model } \end{flushleft}
 \begin{flushleft}\textbf{Hypothesis 3 (H3)} \textit{The investment strategy developed by us based on the model predictions is able to outperform the Buy\&Hold strategy } \end{flushleft}
 \begin{flushleft}\textbf{Hypothesis 4 (H4)} \textit{The model architecture can be successfully applied to more than one company} \end{flushleft}
 \begin{flushleft}\textbf{Hypothesis 5 (H5)} \textit{Adding the possibility of short selling to the investment strategy increases the profitability of our system} \end{flushleft}


\section{Proposed framework}
Our proposed framework is shown in figure X. It consists of two main parts: a predictive model and an investment strategy. The model is a GAN consisting of two competing networks, a discriminator and a generator. The generator tries to generate a prediction as close as possible to the actual closing price, and the discriminator tries to learn to recognise the data artificially generated by the generator and the actual data. We examine three different sets of inputs to the generator: Stock market data only, Stock market data with technical analysis indicators and Stock market data with technical analysis indicators and sentiment analysis results. Our dependent variable is the closing price. The independent variables from the previous 30 days are used to predict it.  Once the model is trained, the generator itself is used for the prediction. The second part is the investment strategy. The prediction of the closing price for the next day is compared with the closing price of today. Based on their difference, our system makes a decision to sell, buy or hold the asset. This chapter will present the detailed components of our framework. 


\subsection{Basic Recurrent Neural Network}

Recurrent neural networks (RNN) is an extended version of artificial neural networks. Its main advantage is having internal memory which allows it to process sequences. Hidden state allows previous outputs to be used as input for further parts of the sequence. This ability to remember previous states makes RNN suitable for time series forecasting. 

\subsection{Gated Recurrent Unit}

Gated Recurrent Unit is an extended version of Recurrent Neural Networks which adress RNN vanishing gradient problem \cite{gru2}.
It's architecture has been proposed by K.Cho in 2014 \cite{gru3}. Its name comes from gating mechanisms which allow the perceptron to choose which information should be saved or forgotten. It is similar to  Long short-term memory networks, yet it lacks an output gate. This difference makes GRU less computationally expensive while maintaining similar or even better performance on smaller datasets. 

\subsection{Convolutional neural network}

Convolutional neural network is another class of artificial neural networks used in our proposed GAN model. It is widely used in many different fields, including computer vision, speech processing, and text processing \cite{cnn1}. One of its main advantages is the ability to identify important features without previous indications. It takes its name from mathematical linear operations called convolution \cite{cnn2}. It has been chosen for our study due to its good differentiating capabilities.

\subsection{Generative adversarial network}

Generative Adversarial networks are a family of neural networks first proposed by J. Godfellow in 2014 \cite{gan1}. The main field in which they are used is computer vision, but since their inception, various modifications of them have been tested in different fields, such as text-to-image translaction and synthetic data generation. They are currently being tested in time series prediction as well.  GAN consists of two neural networks competing against each other in a zero-sum game. The generator tries to generate data as similar to the real data as possible, and the discriminator tries to recognize which data is real and which is generated. In other words, the generator tries to minimise the difference between the true and synthetic distributions, while the discriminator tries to maximise this difference. This can be represented by the following min-max function: 

\begin{center}   GAN min-max function: \end{center}
\begin{equation} \min_G \max_D V(D, G)=
\mathbb{E}_{x\sim p_{data}(x)}[\log D(x)]
+ \mathbb{E}_{z\sim p_z(z)}[\log(1 - D(G(z)))] \end{equation}\\

In classical GAN models, the input to the G generator would be a latent vector derived from N(0, 1). In the model we use, this is replaced by a vector consisting of both the sentiment vector \cite{gan-stock}, data from the stock market and technical analysis indicators for early convergence \cite{gan-stock2}. Its task is to generate a vector G(x) as similar as possible to the original distribution. In our system, due to its ability to deal with time series, the GRU network was chosen as the generator. Our proposed generator consists of three GRU layers containing 1024, 512 and 256 neurons, respectively. Each of them has a recurrent dropout of 0.2. These are followed by three multilayer perceptron (MLP) layers containing 128, 64 and 1 neuron, sequentially \cite{gan-stock}\cite{gan-stock2}. 
The generator loss function is shown as follows: 

\begin{center}   Generator Loss Function:  \end{center}
\begin{equation}
-\frac{1}{m} \sum_{i=1}^{m} \log \left(D\left(G\left(x^{i}\right)\right)\right)
\end{equation}\\

The discriminator in our model is a network composed of a 1-dimensional Convolutional Neural Network. The network was chosen as the discriminator due to its differentiating capabilities. Its task is to discriminate between synthetic and real data. It assigns 1 to the values coming from the true distribution and 0 to the false one. It consists of three convolutional layers. The first contains 32 units and has a kernel size of 3. The second contains 64 units and has a kernel size of 5. The third contains 128 units and has a kernel size of 5. Each has strides of 2 and a Leaky ReaLU activation function with an alpha parameter of 0.1. Then, there is a flatten layer which flattens the input. It is followed by three multilayer perceptron (MLP) layers containing 220, 220 and 1 units, sequentially \cite{gan-stock}\cite{gan-stock2}. 
The discriminator loss function is shown as follows: 
\begin{center}  Discriminator Loss Function: \end{center}
\begin{equation}
-\frac{1}{m} \sum_{i=1}^{m}\left[\log D\left(y^{i}\right)+\log \left(1-D\left(G\left(x^{i}\right)\right)\right)\right]
\end{equation}

The optimizer used for both Generator and Discriminator was ADAM and the learning rate was set to 0.0016.

\subsection{Data Scaling}
Due to the use of neural network based architectures in the study, rescaling of the data was required. For this purpose, min-max normalization was used. The equation for the rescaled value is: 
\begin{equation} x\textsubscript{scaled} = \frac{x - min(x)}{max(x) - min(x)} \end{equation}

\subsection{Train test split}
To avoid overfitting and evaluate models and investment strategies on data that was not used during training, we decided to split our dataset into train dataset and test dataset. 75 \% percent of the dataset has been assigned to train dataset, while the remaining 25 \% to test dataset. Training set was used to train the GAN model and to choose the best parameters for investment strategy. Then, a test dataset was used to check our findings on data that hasn't been seen by our system before.


\subsection{Investment Strategy}

An investment strategy based on the model predictions was also developed in our study. This strategy makes several important assumptions: 
 \begin{flushleft}\textit{- on a given day t we are able to buy at the very end of the day an asset at close price} \end{flushleft}
 \begin{flushleft}\textit{- at the start of our experiment we have 1000 dollars}  \end{flushleft}
 \begin{flushleft}\textit{- it is possible to enter a short trade on the asset.} \end{flushleft}
\begin{flushleft}\textit{- we use all our cash resources for each trade } \end{flushleft}
\begin{flushleft}\textit{- trade fee is equal to 0.0007\footnote{Transaction fee comes from NASDAQ transaction fee in IterativeBrokers (https://www.interactivebrokers.com/en/index.php?f=948)} \$ } \end{flushleft}

Its main idea is to compare the prediction of the final price of today with the real price of the next day. We also take into account the presence of transaction fees. For this reason our strategy includes a $\alpha$ parameter which ensures that a buy/sell signal is generated when the price prediction of the next day and the previous day differs by $\alpha$. As a result, trades are executed less frequent and more profitable. It also means that funds are not consumed by transaction fees. There are different $\alpha$ parameters for the buy and sell signal. We also check the behaviour of the model when allowing short trades. In this case, short trade is entered along with a sell signal. 
Different versions of it were tested and then the best one was selected for each company.
The diffrence of prices was calculated during following formula:

\begin{equation}  diff_{t+1} = \hat{y}_{t+1} - y_{t1} \end{equation}
where:
\begin{conditions}
diff_{t+1} & calculated diffrence \\ 
 \hat{y}_{t+1}     &   predicted price of asset in next day\\
 y_{t}     &  actual price of asset in current day\\   
\end{conditions}

 \begin{flushleft} Then, the buy/selll/hold signal is chosen using following equation: \end{flushleft} 
\begin{equation}
signal =  \left\{\begin{array}{ll} buy & diff_{t+1} >  y_{t} \cdot  \alpha_{buy} \div 100 \\ sell & diff_{t+1} <  -(y_{t} \cdot  \alpha_{sell} \div 100)\\  hold & otherwise \end{array}\right.
\end{equation}
where:
\begin{conditions}
diff_{t+1} & calculated diffrence \\ 
\alpha_{buy}     &  alpha parameter for buy signal \\   
\alpha_{sell}     &  alpha parameter for sell signal\\   
\end{conditions}

\section{Data}
\subsection{Stock Data}
In our analysis we decided to train the algorithm on stocks of 4 large companies from the gaming industry: Electronic Arts, Ubisoft, Take-Two Interactive Software, Activision Blizzard, called in the later part of the paper by their stock ticker: EA, UBSFY, TTWO, ATVI. Są dwie przyczyny wybrania tych czterek konkrentych spółek.The first is that they are one of the biggest companies in the gaming industry. The second is that we are trying to prove that our system is generalising. This sector was chosen due to the fact that the study takes into account the influence of sentiment analysis - one of our hypotheses is that the opinions of Reddit users have a particularly significant impact on valuations of a given company in the gaming industry.  The selection of more than one company was influenced by making sure that the algorithm was reproducible and universal. These companies are among the leaders in the gaming industry. The choice of only U.S. companies was due to a possible language barrier when analyzing the sentiment of companies that produce games primarily for the Asian market. Due to the use of one-day intervals, limitations of Reddit's API (Application Programming Interface), and limitations of computing power, the analysis was conducted only over a nearly three-year period: from 01/01/2019 to 31/10/2021. Data is analyzed in 1 day periods. The data comes from the US NASDAQ stock market and was retrieved using the yahoo finance library for the python language. 

\begin{table}[hbt!]
\centering
\caption{Descriptive statistics for chosen companies}
\begin{tabular}{llrrrr}
\toprule
       & Company &          ATVI &           EA &         TTWO &        UBSFY \\
\midrule
Open & count &      178.0000 &     178.0000 &     178.0000 &     178.0000 \\
       & mean &       83.8875 &     139.3584 &     170.9768 &      12.8549 \\
       & std &       11.4400 &       5.3196 &      10.6307 &       1.8079 \\
       & min &       56.9778 &     120.4896 &     145.9100 &       9.0500 \\
       & 25\% &       77.6000 &     137.5287 &     163.1550 &      11.1675 \\
       & 50\% &       83.5100 &     140.7925 &     171.3150 &      12.9601 \\
       & 75\% &       93.5850 &     142.8999 &     178.8850 &      14.2100 \\
       & max &       98.8600 &     148.5431 &     192.2600 &      16.0996 \\
High & count &      178.0000 &     178.0000 &     178.0000 &     178.0000 \\
       & mean &       84.7118 &     140.8630 &     172.9497 &      12.9261 \\
       & std &       11.4490 &       5.2717 &      10.6649 &       1.8045 \\
       & min &       57.4400 &     123.5934 &     147.1000 &       9.2700 \\
       & 25\% &       78.3725 &     139.5444 &     164.6825 &      11.2589 \\
       & 50\% &       84.4625 &     142.1893 &     173.3464 &      13.1050 \\
       & 75\% &       94.7925 &     144.0306 &     182.2375 &      14.2800 \\
       & max &       99.4590 &     148.5531 &     195.8250 &      16.1800 \\
Low & count &      178.0000 &     178.0000 &     178.0000 &     178.0000 \\
       & mean &       82.7644 &     137.7310 &     168.8698 &      12.7556 \\
       & std &       11.4891 &       5.4720 &      10.3637 &       1.8032 \\
       & min &       56.4000 &     119.9184 &     144.5810 &       9.0500 \\
       & 25\% &       76.6675 &     136.2402 &     160.5650 &      11.0476 \\
       & 50\% &       82.6550 &     139.2077 &     169.4100 &      12.8400 \\
       & 75\% &       92.7575 &     141.2427 &     176.5900 &      14.1275 \\
       & max &       97.6100 &     145.7801 &     186.4200 &      16.0500 \\
Close & count &      178.0000 &     178.0000 &     178.0000 &     178.0000 \\
       & mean &       83.6483 &     139.1895 &     170.8772 &      12.8366 \\
       & std &       11.4663 &       5.4726 &      10.5099 &       1.8054 \\
       & min &       57.2800 &     120.0682 &     145.2500 &       9.1400 \\
       & 25\% &       77.3650 &     137.8033 &     162.8950 &      11.1625 \\
       & 50\% &       83.4700 &     140.6691 &     171.2800 &      12.9750 \\
       & 75\% &       93.4025 &     142.6030 &     178.9275 &      14.2174 \\
       & max &       99.1800 &     148.1741 &     192.9100 &      16.1300 \\
Volume & count &      178.0000 &     178.0000 &     178.0000 &     178.0000 \\
       & mean &  8236266.1966 & 2473996.9831 & 1276613.7528 &  169367.5056 \\
       & std &  5413813.4429 & 1073436.8484 &  669500.1064 &  278989.7716 \\
       & min &  2596873.0000 & 1016396.0000 &  559480.0000 &   16495.0000 \\
       & 25\% &  5105944.5000 & 1766566.2500 &  876545.7500 &   46863.2500 \\
       & 50\% &  6667305.0000 & 2181122.5000 & 1095232.0000 &   69654.5000 \\
       & 75\% &  9477788.5000 & 2854991.5000 & 1422011.2500 &  148630.0000 \\
       & max & 43753567.0000 & 8344344.0000 & 5935523.0000 & 1833827.0000 \\
\bottomrule
\end{tabular}
\end{table}

\clearpage

\subsection{Text Data}
As it was mentioned before sentiment analysis is the part of the research. Due to the fact that we considered in the study only the companies from the gaming industry, we decided to use data coming from the reddit.com portal, which is one of the largest forums in the world. It also has a feature that helps to obtain data for our study - it is divided into parts, the so-called subreddits, which gather people interested in a particular topic. In the study the r/Games subreddit is chosen as a main source of text data. This forum has 3.1 milion users and has milions of comments about games. 

Next, for each company selected by us, keywords were chosen to retrieve the data. The keywords were the names of the most popular game series of a particular publisher and the publisher's name itself. This allows us to analyse how the opinions of users on games created by a given company looked on a given day. Next, using the psaw library for python, we found all the comments that had been posted over a predefined period of time on the r/Games subreddit containing the keywords mentioned below. 

\begin{table}[hbt!]
\centering
\caption{Key words chosen for each company}
\begin{tabular}{lllll}
\toprule
{} &              EA &                 TTWO &              UBSFY &                 ATVI \\
\midrule
0  &              EA &             Take Two &            Ubisoft &             Blizzard \\
1  &            Fifa &               NBA 2K &    Assasin's Creed &            Starcraft \\
2  &        The Sims &           Battleborn &                 AC &             Warcraft \\
3  &  Need for Speed &             BioShock &            Far Cry &            Overwatch \\
4  &             NFL &          Borderlands &         Watch Dogs &               Diablo \\
5  &            Apex &               Evolve &  Rainbow Six Siege &    World of Warcraft \\
6  &     Battlefield &                Mafia &          Wildlands &          Hearthstone \\
7  &       Bejeweled &         Civilization &          For Honor &  Heroes of the Storm \\
8  &     Battlefront &         The Darkness &       Tom Clancy's &                  \\
9  &             NBA &                 XCOM &       The Division &                  \\
10 &      Dragon Age &                  WWE &                &                  \\
11 &       Titanfall &                  GTA &                &                  \\
12 &      Dead Space &     Grand Theft Auto &                &                  \\
13 &             &            Max Payne &                &                  \\
14 &             &  Red Dead Redemption &                &                  \\
15 &             &                  RDR &                &                  \\
\bottomrule
\end{tabular}
\end{table}



\subsection{Technical Analysis}
Technical analysis is the most commonly used method in automated trading. Its indicators try to predict the behaviour of a given stock market indicator based on mathematically processed recent observations. It is used in our analysis for several reasons. The first is the ability to obtain early convergence, thus reducing the time to train the model and the resources needed to do so. Moreover, such a wide use of these indicators among other exchange participants allows our system to take into account their possible behaviour caused by the results of technical indicators. In our study, the indicators described below were taken into account. 

\subsubsection{Moving Averages}
Several types of moving averages are used. The first and simplest of them is a simple moving average (SMA). It takes into account equally weighted observations in a given time window. For obvious reasons, one may assume that closer dates may have more significant influence on future price. Therefore, in addition to the SMA, Weighted Moving Average (WMA) and Exponential Moving Average were used. The WMA solves the aforementioned problem by giving more weight to more recent data. EMA works in a similar way, but the price change is not consistent but exponential.

\subsubsection{Bollinger Bands}
Bollinger Bands consist of three bands. The middle one is a moving average. Higher and lower bands are deviated from the middle one by 2 standard deviations up and down respectively. Every of these three bands is used as an input in GAN model.

\subsubsection{Moving Average Convergence Divergence}
Moving Average Convergence Divergence (MACD) consists of two lines. The core of the indicator is the MACD line which is the difference between the 12-period EMA and the 26-period EMA. The second line is the signal line which is a 9-period EMA. Their position relative to each other helps to determine whether the market is oversold or overbought.

\subsection{Sentiment Analysis}
Sentiment analysis is a process of extracting users' feelings and emotions. It is a part of Natural Language Processing. It boils down to trying to determine with a given probability whether a given statement was positive, negative or neutral. Then such predictions are converted into numerical data. There are many different types of models for sentiment analysis, but the vast majority of them are based on machine learning. In our study, the BERT (Bi-Directional Encoder Representations from Transformers) model created by Google researchers was used. 

\subsection{BERT (Bi-Directional Encoder Representations from Transformers)}
BERT is a state-of-the-art NLP model. One of its biggest advantages is taking whole sentences as an input in contrast to traditional NLP models that take one word a time. For this reason it is referred to as Bi-Directional. In this way, he can also learn the context between the words in a sentence.One of BERT's biggest advantages is that it is a semi-supervised model. It is pre-trained on very large sets of non labeled data, learning to fill gaps in the text. In this way it is forced to identify the masked word based on the context. It also uses an attention mechanism that allows it to learn the relationship between words.Then this model can be trained for any task just by adding one extra layer.  All this makes BERT an extremely versatile model and very well suited to sentiment analysis without the need for large computational resources. 


Due to the occurrence of many comments concerning a single company on a given day, it was necessary to group them. All comments containing a key word related to a given company were combined into one matrix. Their sentiment was then calculated. This data was aggregated by day of creation into the following vector for each day: 
\begin{center}   Sentiment vector for day \emph{i}:\end{center}
\begin{equation} [n, \mu, \sigma, med, Q_1, Q_3] \end{equation}
where:
\begin{conditions}
 n     &  number of comments related to chosen company on day \emph{i}\\
 med     &  median of all values related to chosen company on day \emph{i} \\  
\mu    &  mean of all values related to chosen company on day \emph{i} \\    
\sigma    &  standard deviation of all values related to chosen company on day \emph{i} \\   
Q_1     &  first quantile of all values related to chosen company on day \emph{i} \\   
Q_3     &  third of all values related to chosen company on day \emph{i} \\   
\end{conditions}

\subsection{Evaluation of the model}

Evaluation of the model is going to be done using two metrics:  \\ 

\begin{center}   Root Mean Square Error (RMSE):  \end{center}
\begin{equation}  RMSE = \sqrt{(\frac{1}{n})\sum_{i=1}^{n}(y_{i} - x_{i})^{2}} \end{equation}
where:
\begin{conditions}
 x_i     &  true value \\
 y_i     &  predicted value \\   
 n &  number of observation
\end{conditions}

\begin{center}  Mean Absolute Error (MAE): \end{center}
\begin{equation} MAE =(\frac{1}{n})\sum_{i=1}^{n}\left | y_{i} - x_{i} \right | \end{equation}
where:
\begin{conditions}
 x_i     &  true value \\
 y_i     &  predicted value \\   
 n &  number of observation
\end{conditions}

\section{Results of framework}

\subsection{GAN results}

The model was trained using the Python language and its frameworks - tensorflow \cite{tensorflow}  and keras \cite{keras}\cite{keras-2}. The hardware used for training consisted of an Nvidia GeForce GTX 1070 graphics card, an Intel i5-6600 processor, and 16 GB of RAM. The training was performed together with the use of CUDA cores. Each training was run with an epoch parameter of 1000. Hyper parameter optimisation was also performed,  however, due to the limitation of computing power, it could not be extensive tak jak na początku to było planowane.Our original plan was to check parameters such as the number of layers, the number of neurons in the different layers and the activation functions in both the discriminator and the generator. Due to the previously mentioned limitations only the behaviour of the network after removing the MLP layers from both the generator and the discriminator was checked. This did not give the desired results, therefore in the further part of the study the parameters proposed by P. Sonkiya et al. were used. The behaviour of the model with different types of input data for the generator was also checked. We have removed  the vector containing sentiment statistics and/or technical analysis indicators. By far the best model was found to be the one without sentiment and with technical analysis. One reason why the use of sentiment may not be correct is the noise and data impurity from reddit. However, this topic needs further exploration with more computational resources. 

\begin{table}[H]
\centering
\caption{Error metrics for chosen companies}
\begin{tabular}{lrrrr}
\toprule
{} &       ATVI &          EA &        TTWO &      UBSFY \\
\midrule
MAE              &   1.358500 &    1.342700 &    3.540500 &   0.257800 \\
RMSE             &   1.886700 &    1.895600 &    4.506900 &   0.337600 \\
Close Price Mean &  83.648342 &  139.189536 &  170.877192 &  12.836576 \\
\bottomrule
\end{tabular}
\end{table}




\begin{figure}[H]
\begin{adjustbox}{width=1.3\textwidth,center}
\input{ATVI_prices.pgf}
\end{adjustbox}
\end{figure}

\begin{figure}[H]
\begin{adjustbox}{width=1.3\textwidth,center}
\input{ATVI_loss.pgf}
\end{adjustbox}
\end{figure}

\begin{figure}[H]
\begin{adjustbox}{width=1.3\textwidth,center}
\input{EA_prices.pgf}
\end{adjustbox}
\end{figure}

\begin{figure}[H]
\begin{adjustbox}{width=1.3\textwidth,center}
\input{EA_loss.pgf}
\end{adjustbox}
\end{figure}

\begin{figure}[H]
\begin{adjustbox}{width=1.3\textwidth,center}
\input{TTWO_prices.pgf}
\end{adjustbox}
\end{figure}

\begin{figure}[H]
\begin{adjustbox}{width=1.3\textwidth,center}
\input{TTWO_loss.pgf}
\end{adjustbox}
\end{figure}

\begin{figure}[H]
\begin{adjustbox}{width=1.3\textwidth,center}
\input{UBSFY_prices.pgf}
\end{adjustbox}
\end{figure}

\begin{figure}[H]
\begin{adjustbox}{width=1.3\textwidth,center}
\input{UBSFY_loss.pgf}
\end{adjustbox}
\end{figure}

The results of model analysis on the test dataset are presented in Table 3, where we compare previously selected metrics - MAE and RMSE together with the average share price of a given company in the test dataset. The mean has been included here due to the relativity of error.  For each of the companies, the MAE does not exceed 2.1 \% of the average price value over the test period. The best performer here is EA, for which this value is 0.95\%.  In the case of this company, the model performed best when the epoch parameter was changed from 1000 to 500. Another important fact is the crossing of the prediction and actual price lines on each of the graphs showing the differences between predicted value and actual value. This is crucial, because if these lines did not intersect, it would mean that the model constantly predicts too high or too low value. Such a situation would make it impossible to create a working investment strategy. \\

The loss functions of each of the models are also worth analysing. They represent the change in the value of the generator and discriminator loss functions over time. It is easy to see how they compete with each other. It can also be seen that changes in the loss functions occur simultaneously, i.e. if the generator loss changes then the discriminator loss also changes. In any case, around epochs 100-300 there is a period of strong instability, characterised by very strong changes in both loss functions. It usually lasts several tens of epochs and ends with stabilisation of both functions. In 3 out of 4 cases the period of severe instability occurs only once, the exception being UBSFY where such periods occur twice. After the initial periods of instability the loss function becomes stable, only experiencing subtle changes once in a while. Once the loss function has stabilised, such a model is ready to be used in an investment strategy. It follows that with our proposed framework it is necessary to train the model for at least 400 epochs. 

\subsection{Investment strategy results}
Once satisfactory models were created, the results of our investment strategy were analysed. 
\begin{table}[H]
\centering
\caption{Signal hyper parameters}
\begin{tabular}{lrrl}
\toprule
  $\alpha_{buy}$ &  $\alpha_{sell}$  & if\_short \\
\midrule
        0.0 &           0.0 &     True \\
         0.2 &           0.2 &    False \\
         0.4 &           0.4  \\
         0.6 &           0.6  \\
          0.8 &           0.8  \\
          1.0 &           1.0  \\
          1.2 &           1.2  \\
          1.4 &           1.4  \\
          1.6 &           1.6  \\
          1.8 &           1.8  \\
         2.0 &           2.0  \\
          2.2 &           2.2  \\
          2.4 &           2.4  \\
          2.6 &           2.6  \\
          2.8 &           2.8  \\
          3.0 &           3.0  \\
          3.2 &           3.2  \\
          3.4 &           3.4  \\
          3.6 &           3.6  \\
          3.8 &           3.8  \\
\bottomrule
\end{tabular}
\end{table}

To select the best possible set of parameters, hyperparameter optimisation was carried out. In order to avoid matching the strategy with the test results, this was performed on the training data. Using the parameters shown in Table 4, all their possible permutations were created. The algorithm for Backtesting was then run with each possible permutation and the set of parameters maximising the balance at the end of the training period was found. The selected parameters for each company are presented in Table 5. 

\begin{table}[H]
\centering
\caption{Best strategy hyper parameters}
\begin{tabular}{llrrl}
\toprule
 ticker &  $\alpha_{buy}$ &  $\alpha_{sell}$  &  if\_short \\
\midrule
  UBSFY &          1.8 &           1.4 &      True \\
     EA &          2.6 &           0.6 &      True \\
   TTWO &          1.6 &           0.4 &      True \\
   ATVI &          0.0 &           2.2 &      True \\
\bottomrule
\end{tabular}
\end{table}


The parameters for the investment strategy found in this way are subject to some bias. It is likely that the predictions of the test set would be less precise than te train set. Therefore, an additional set of parameters was arbitrarily chosen. The parameter $\alpha_{buy}$  is equal to 2, $\alpha_{sell}$  to 0. A short selling option has also been made available.  A high top-cut-off was chosen with a conservative approach to buying, making trades less frequent and therefore funds are not used up by transaction fees. A zero down-cut-off will instead give an instant sell if the model prediction is higher than the previous price. This is intended to minimise losses. Since these parameters are chosen purely on the basis of our beliefs they may require deeper analysis. One idea is to split data into one more dataset that would be used exclusively to select parameters for a trading strategy that would be tested on a real test set that our system has not seen. 

\begin{table}[H]
\centering
\caption{Diffrent strategies balance at the end of test time peroid}
\begin{tabular}{llrrr}
\toprule
 ticker &  Buy and Hold &  Proposed strategy with  &  Proposed strategy with  \\
  &        &                                        parameters found in  &                                           arbitrary chosen parameters \\
  &        &                                         hyperparameter optymalization &                                            \\

 \midrule
  UBSFY &       631.839 \$&                                            878.008 \$ &                                           1258.925 \$\\
    EA &       935.412 \$&                                           1006.768 \$&                                           1079.715 \$\\
   TTWO &       944.500 \$&                                            876.744 \$&                                            870.907 \$\\
   ATVI &       660.481 \$&                                            631.715 \$&                                           1110.533 \$\\
\bottomrule
\end{tabular}
\end{table}



\begin{figure}[H]
\begin{adjustbox}{width=1.3\textwidth,center}
\input{UBSFY_strategy.pgf}
\end{adjustbox}
\end{figure}

\begin{figure}[H]
\begin{adjustbox}{width=1.3\textwidth,center}
\input{EA_strategy.pgf}
\end{adjustbox}
\end{figure}

\begin{figure}[H]
\begin{adjustbox}{width=1.3\textwidth,center}
\input{TTWO_strategy.pgf}
\end{adjustbox}
\end{figure}

\begin{figure}[H]
\begin{adjustbox}{width=1.3\textwidth,center}
\input{ATVI_strategy.pgf}
\end{adjustbox}
\end{figure}


 As can be seen in the charts above, in 3 out of 4 cases the strategy we presented outperformed the Buy and Hold strategy. Interestingly, the arbitrarily chosen parameters for the investment strategy turned out to be better than those found by hyperparameter optimization. However, this agrees with our assumption that due to differences in model prediction on the training and test sets, these parameters may not be optimal on data that our system has not seen. The worst results were obtained for TTWO. However, this is understandable, because already at the level of the model itself it was clear that MAE compared to average price values is the highest there. This means that this model is not good enough and needs further research. It is important to note that each of these companies was in a downtrend. The Buy and Hold strategy has in each case recorded losses. This makes it all the more optimistic that 3 of the 4 assets made significant gains. 

\section{Conclusion}

The aim of our work was to test the feasibility of using the GAN network and the BERT model to predict the valuation of four listed companies. The BERT model was used to analyse the sentiment of comments about a given company and its games from the reddit.com. These data along with technical analysis indicators and stock market data were then used as explanatory variables for the GAN model. GAN models without sentiment analysis and/or technical analysis indicators were also tested. Then, using the predictions from our model, we created a system to decide on a buy or sell signal. An optimisation of the hyperparameters in this system was carried out in order to find profit-optimising parameters. For 3 of the 4 companies we managed to create a strategy that significantly outperforms the buy and hold approach. \\

The study tested the hypotheses raised in the introduction. \\ 

The first hypothesis \textit{(H1) Using data from sentiment analysis positively influences the behaviour of the model} has to be rejected since we haven't managed to create well performing model without throwing sentiment data away. Their use made our model performed much worse. This may have been due to flaws in the data retrieval API and noise in the text data. \\

The second hypothesis\textit{ (H2) The use of data from technical analysis positively influences the behaviour of the model}, holds due to the fact that technical analysis improved every of our models. This matches our suspicion that these indicators can provide our model with additional information and accelerate its training. \\

The third hypothesis\textit{ (H3) The investment strategy us based on the model predictions is able to outperform the Buy\&Hold strategy} holds for 3 out of 4 companies that we have tested. The reason for this is that the TTWO model was surely the worst one and it needs further research. Since we have proven that our system can outperform buy and hold approach, H3 cannot be rejected.\\

The fourth hypothesis\textit{H (H4) The model architecture can be successfully applied to more than one company}, holds due to the fact that we were able to apply it to 3 out of 4 tested companies. \\


Finally, the fifth hypothesis\textit{ (H5) Adding the possibility of short selling to the investment strategy increases the profitability of our system}, holds due to the fact that that strategies with ability to short selling performed better \\

In conclusion, the main finding is the confirmation of the possibility to use GAN networks to create an effective system that allows us to outperform Buy and Hold strategy and to make profit despite the fall in the price of an asset. The study has also shown that these networks have their problems and there is a great scope for further research. The main ones are their instability during training and the unclearness of when to stop training.  Better results could also be achieved with a more sophisticated investment strategy. Nevertheless, the results are promising and proves that this system could have potential business applications.

\section{Intentional self-criticism}

One of the problems posed by the existence of two loss functions is the difficulty of determining when a model is trained. Due to the fact that the two networks compete with each other, it is impossible to say unambiguously what values of the two loss functions characterise the best model. This is also a considerable difficulty in optimising hyperparameters, as it is difficult to determine at which point training should be stopped. \\

Another problem is the different results with the same parameters. Neural networks are by definition not deterministic, as stochastic gradient descent and random assignment of initial weights can affect random network behaviour and different results with the same parameters. They become deterministic after they have been trained and all weight are fixed. This effect is further compounded by the fact of using two networks competing with each other and having two separate loss functions. Further analysis can focus on stabilising our system so that in business use it is possible to retrain it on new data without the risk of breaking the whole model. \\

The investment strategy also needs further analysis. In our study, a very simple method was used, which involves trading the entire funds on each trade. In order to minimise the risk of loss in the event of a sudden fluctuation, the system could be changed in the future to dose the amount of funds allocated to buy or sell transactions depending on the model's confidence in the price change. 

\begin{thebibliography}{9}
\bibitem{ml-in-stock}
A. Prasad, A. Seetharaman, Importance of Machine Learning in Making Investment Decision in Stock Market, 2021
\bibitem{ml-compute}
M. Vijh et al, Stock Closing Price Prediction using Machine Learning Techniques, 2020
\bibitem{ml-studies}
T. Strader, Machine Learning Stock Market Prediction Studies: Review and Research Directions, 2020
\bibitem{gru1}
J. Chung, C. Gulcehre, K. Cho, Y. Bengio, Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, 2014
\bibitem{gru2}
P. Dey et al, Comparative Analysis of Recurrent Neural Networks in Stock Price Prediction for Different Frequency Domains, 2021 
\bibitem{gru3} 
K. Cho et al, On the Properties of Neural Machine Translation: Encoder-Decoder Approaches, 2014
\bibitem{cnn1}
L. Alzubaidi et al, Review of deep learning: concepts, CNN architectures, challenges, applications, future directions, 2021, 14
\bibitem{cnn2}
R. Yamashita, Convolutional neural networks: an overview and application in radiology, 2018, 612
\bibitem{gan1}
J. Goodfellow et al, Generative Adversarial Nets, 2014
\bibitem{time-gan}
J. Yoon, D. Jarret, M. Schaar, Time-series Generative Adversarial Networks, 2019
\bibitem{s-gan}
P. Sonkiya, V. Bajpai, A. Bansal, Stock price prediction using BERT and GAN, 2021
\bibitem{gan-stock}
A. Kumar et al, Generative Adversarial Network (GAN) and Enhanced Root Mean Square Error (ERMSE): Deep Learning for Stock Price Movement Prediction, 2021
\bibitem{gan-stock2}
H. Lin et al, Stock price prediction using Generative Adversarial Networks, 2021 
\bibitem{tensorflow}
M. Abadi et al, Tensorflow: A system for large-scale machine learning, 2016
\bibitem{keras}
F Chollet et al, Keras, 2015
\bibitem{keras-2}
A. Gulli,  Deep learning with Keras, 2017
\bibitem{lstm-btc}
J. Michańków, P. Sakowski, R. Ślepaczuk, LSTM in Algorithmic Investment Strategies on BTC and S\&P500 Index, 2022
\bibitem{gan-cnn}
A. Staffini, Stock Price Forecasting by a Deep Convolutional Generative Adversarial Network, 2022
\bibitem{gan-zhang}
Zhang et al, Stock Market Prediction Based on Generative Adversarial Network, 2019
\bibitem{gan-Jjang}
J. Jiang, Stock Market Prediction Based on SF-GAN Network, 2021
\bibitem{gan-lin}
H. Lin et al, Stock price prediction using Generative Adversarial Networks, 2021
\cite{at}
B Huang et al, Automated trading systems statistical and machine learning methods and hardware implementation: a survey

\end{thebibliography}
\end{document}